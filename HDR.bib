@inproceedings{usevichCharacterizationFiniteSignals2018,
  title = {Characterization of {{Finite Signals}} with {{Low-Rank Stft}}},
  booktitle = {2018 {{IEEE Statistical Signal Processing Workshop}} ({{SSP}})},
  author = {Usevich, Konstantin and Emiya, Valentin and Brie, David and Chaux, Caroline},
  year = {2018},
  month = jun,
  pages = {393--397},
  publisher = {IEEE},
  address = {Freiburg},
  doi = {10.1109/SSP.2018.8450745},
  urldate = {2023-07-07},
  abstract = {The goal of this paper is characterize finite-length signals that have a low-rank short-time Fourier transform. By using the connection with Hankel matrices, we give a complete answer for maximal overlap, where the class of signals includes products of complex exponentials and polynomials. For the general case, we show that such signals are much more diverse.},
  isbn = {978-1-5386-1571-3},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/4VYUXQ92/Usevich et al. - 2018 - Characterization of Finite Signals with Low-Rank S.pdf}
}

@article{cohenALMOSTSUREIDENTIFIABILITY,
  title = {{{ON ALMOST SURE IDENTIFIABILITY OF NON MULTILINEAR TENSOR DECOMPOSITION}}},
  author = {Cohen, Jeremy E and Comon, Pierre},
  abstract = {Uniqueness of tensor decompositions is of crucial importance in numerous engineering applications. Extensive work in algebraic geometry has given various bounds involving tensor rank and dimensions to ensure generic identifiability. However, most of this work is hardly accessible to non-specialists, and does not apply to non-multilinear models. In this paper, we present another approach, using the Jacobian of the model. The latter sheds a new light on bounds and exceptions previously obtained. Finally, the method proposed is applied to a non-multilinear decomposition used in fluorescence spectrometry, which permits to state generic local identifiability.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/7LTDAT9G/Cohen and Comon - ON ALMOST SURE IDENTIFIABILITY OF NON MULTILINEAR .pdf}
}

@article{cohenAnalyseGrandesDonnees,
  title = {{Analyse de grandes donn{\'e}es tensorielles coupl{\'e}es}},
  author = {Cohen, J{\'e}r{\'e}my E and Farias, Rodrigo CABRAL and Comon, Pierre},
  abstract = {Tensor data analysis is presently one of the hot topics in signal processing. New measurement tools allow huge amount of multivariate complementary data to be collected on the same physical phenomenon, a property very well exploited by tensor algebra. In this paper, we treat the big data problem when tensorial data are coupled along one factor. A joint compression scheme and new coupled decomposition algorithms are introduced and demonstrated on synthetic data.},
  langid = {french},
  file = {/home/cohen/Travail/Articles/storage/C66IF7AF/Cohen et al. - Analyse de grandes données tensorielles couplées.pdf}
}

@incollection{cabralfariasJointDecompositionsFlexible2015,
  title = {Joint {{Decompositions}} with {{Flexible Couplings}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Cabral Farias, Rodrigo and Cohen, J{\'e}r{\'e}my Emile and Jutten, Christian and Comon, Pierre},
  editor = {Vincent, Emmanuel and Yeredor, Arie and Koldovsk{\'y}, Zbyn{\v e}k and Tichavsk{\'y}, Petr},
  year = {2015},
  volume = {9237},
  pages = {119--126},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-22482-4_14},
  urldate = {2023-07-07},
  abstract = {A Bayesian framework is proposed to define flexible coupling models for joint decompositions of data sets. Under this framework, a solution to the joint decomposition can be cast in terms of a maximum a posteriori estimator. Examples of joint posterior distributions are provided, including general Gaussian priors and non Gaussian coupling priors. Then simulations are reported and show the effectiveness of this approach to fuse information from data sets, which are inherently of different size due to different time resolution of the measurement devices.},
  isbn = {978-3-319-22481-7 978-3-319-22482-4},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/QC2T3YMH/Cabral Farias et al. - 2015 - Joint Decompositions with Flexible Couplings.pdf}
}

@article{veganzonesNonnegativeTensorCP2016,
  title = {Nonnegative {{Tensor CP Decomposition}} of {{Hyperspectral Data}}},
  author = {Veganzones, Miguel A. and Cohen, Jeremy E. and Cabral Farias, Rodrigo and Chanussot, Jocelyn and Comon, Pierre},
  year = {2016},
  month = may,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {54},
  number = {5},
  pages = {2577--2588},
  issn = {0196-2892, 1558-0644},
  doi = {10.1109/TGRS.2015.2503737},
  urldate = {2023-07-07},
  abstract = {New hyperspectral missions will collect huge amounts of hyperspectral data. Besides, it is possible now to acquire time series and multiangular hyperspectral images. The process and analysis of these big data collections will require common hyperspectral techniques to be adapted or reformulated. The tensor decomposition, a.k.a. multiway analysis, is a technique to decompose multiway arrays, that is, hypermatrices with more than two dimensions (ways). Hyperspectral time series and multiangular acquisitions can be represented as a 3-way tensor. Here, we apply Canonical Polyadic tensor decomposition techniques to the blind analysis of hyperspectral big data. In order to do so, we use a novel compression-based nonnegative CP decomposition. We show that the proposed methodology can be interpreted as multilinear blind spectral unmixing, a higher order extension of the widely known spectral unmixing. In the proposed approach, the big hyperspectral tensor is decomposed in three sets of factors which can be interpreted as spectral signatures, their spatial distribution and temporal/angular changes. We provide experimental validation using a study case of the snow coverage of the French Alps during the snow season.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/8WUVE35X/Veganzones et al. - 2016 - Nonnegative Tensor CP Decomposition of Hyperspectr.pdf}
}

@inproceedings{veganzonesMultilinearSpectralUnmixing2015,
  title = {Multilinear Spectral Unmixing of Hyperspectral Multiangle Images},
  booktitle = {2015 23rd {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Veganzones, M. A. and Cohen, J. and Farias, R. Cabrai and Marrero, R. and Chanussot, J. and Comon, P.},
  year = {2015},
  month = aug,
  pages = {744--748},
  publisher = {IEEE},
  address = {Nice},
  doi = {10.1109/EUSIPCO.2015.7362482},
  urldate = {2023-07-07},
  abstract = {Spectral unmixing is one of the most important and studied topics in hyperspectral image analysis. By means of spectral unmixing it is possible to decompose a hyperspectral image in its spectral components, the so-called endmembers, and their respective fractional spatial distributions, so-called abundance maps. New hyperspectral missions will allow to acquire hyperspectral images in new ways, for instance, in temporal series or in multiangular acquisitions. Working with these incoming huge databases of multiway hyperspectral images will raise new challenges to the hyperspectral community. Here, we propose the use of compression-based nonnegative tensor canonical polyadic (CP) decompositions to analyze this kind of datasets. Furthermore, we show that the nonnegative CP decomposition could be understood as a multilinear spectral unmixing technique. We evaluate the proposed approach by means of Mars synthetic datasets built upon multiangular in-lab hyperspectral acquisitions.},
  isbn = {978-0-9928626-3-3},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/4XZIDLK3/Veganzones et al. - 2015 - Multilinear spectral unmixing of hyperspectral mul.pdf}
}

@article{cohenFastDecompositionLarge2015,
  title = {Fast {{Decomposition}} of {{Large Nonnegative Tensors}}},
  author = {Cohen, Jeremy E. and Farias, Rodrigo Cabral and Comon, Pierre},
  year = {2015},
  month = jul,
  journal = {IEEE Signal Processing Letters},
  volume = {22},
  number = {7},
  pages = {862--866},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2014.2374838},
  urldate = {2023-07-07},
  abstract = {In Signal processing, tensor decompositions have gained in popularity this last decade. In the meantime, the volume of data to be processed has drastically increased. This calls for novel methods to handle Big Data tensors. Since most of these huge data are issued from physical measurements, which are intrinsically real nonnegative, being able to compress nonnegative tensors has become mandatory. Following recent works on HOSVD compression for Big Data, we detail solutions to decompose a nonnegative tensor into decomposable terms in a compressed domain.},
  langid = {english},
  keywords = {CoheCC15:spl nonnegative compression},
  file = {/home/cohen/Travail/Articles/storage/2GGW2RK7/Cohen et al. - 2015 - Fast Decomposition of Large Nonnegative Tensors.pdf}
}

@inproceedings{cohenJointTensorCompression2016,
  title = {Joint Tensor Compression for Coupled Canonical Polyadic Decompositions},
  booktitle = {2016 24th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Cohen, Jeremy Emile and Farias, Rodrigo Cabral and Comon, Pierre},
  year = {2016},
  month = aug,
  pages = {2285--2289},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/EUSIPCO.2016.7760656},
  urldate = {2023-07-07},
  abstract = {To deal with large multimodal datasets, coupled canonical polyadic decompositions are used as an approximation model. In this paper, a joint compression scheme is introduced to reduce the dimensions of the dataset. Joint compression allows to solve the approximation problem in a compressed domain using standard coupled decomposition algorithms. Computational complexity required to obtain the coupled decomposition is therefore reduced. Also, we propose to approximate the update of the coupled factor by a simple weighted average of the independent updates of the coupled factors. The proposed approach and its simplified version are tested with synthetic data and we show that both do not incur substantial loss in approximation performance.},
  isbn = {978-0-9928626-5-7},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/5AHCSHS7/Cohen et al. - 2016 - Joint tensor compression for coupled canonical pol.pdf}
}

@article{cabralfariasExploringMultimodalData2016,
  title = {Exploring {{Multimodal Data Fusion Through Joint Decompositions}} with {{Flexible Couplings}}},
  author = {Cabral Farias, Rodrigo and Cohen, Jeremy Emile and Comon, Pierre},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {64},
  number = {18},
  pages = {4830--4844},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2016.2576425},
  urldate = {2023-07-07},
  abstract = {A Bayesian framework is proposed to define flexible coupling models for joint tensor decompositions of multiple data sets. Under this framework, a natural formulation of the data fusion problem is to cast it in terms of a joint maximum a posteriori (MAP) estimator. Data driven scenarios of joint posterior distributions are provided, including general Gaussian priors and non Gaussian coupling priors. We present and discuss implementation issues of algorithms used to obtain the joint MAP estimator. We also show how this framework can be adapted to tackle the problem of joint decompositions of large datasets. In the case of a conditional Gaussian coupling with a linear transformation, we give theoretical bounds on the data fusion performance using the Bayesian Crame{\textasciiacute}r-Rao bound. Simulations are reported for hybrid coupling models ranging from simple additive Gaussian models, to Gamma-type models with positive variables and to the coupling of data sets which are inherently of different size due to different resolution of the measurement devices.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/6JW98REN/Cabral Farias et al. - 2016 - Exploring Multimodal Data Fusion Through Joint Dec.pdf}
}

@inproceedings{veganzonesNonnegativeCPDecomposition2016,
  title = {Nonnegative {{CP}} Decomposition of Multiangle Hyperspectral Data: {{A}} Case Study on {{CRISM}} Observations of {{Martian ICY}} Surface},
  shorttitle = {Nonnegative {{CP}} Decomposition of Multiangle Hyperspectral Data},
  booktitle = {2016 8th {{Workshop}} on {{Hyperspectral Image}} and {{Signal Processing}}: {{Evolution}} in {{Remote Sensing}} ({{WHISPERS}})},
  author = {Veganzones, M. A. and Doute, S. and Cohen, J. E. and Farias, R. Cabrai and Chanussot, J. and Comon, P.},
  year = {2016},
  month = aug,
  pages = {1--5},
  publisher = {IEEE},
  address = {Los Angeles, CA, USA},
  doi = {10.1109/WHISPERS.2016.8071723},
  urldate = {2023-07-07},
  abstract = {The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) sensor aboard the Mars Reconnaissance Orbiter takes hyperspectral multi-angle acquisitions of Martian surface from the top of the atmosphere (TOA) on visible and infrared wavelengths. The Multiangle Approach for Retrieval of Surface Reflectance from CRISM Observations (MARS-ReCO) defined an innovative TOA radiance model and inversion scheme aimed at correcting for aerosols effects taking advantage of the near-simultaneous multiangle CRISM observations. Here, we aim to provide validation evidence of MARS-ReCO by unmixing the estimated multi-angle bidirectional reflectance (BRF) from highly reflective and anisotropic icy surfaces at high latitudes with grazing illumination, using a nonnegative CP decomposition. Obtained results are in accordance with other complementary studies, which compose a collaboration effort to validate MARS-ReCO through the cross-validation of different techniques in the absence of ground truth.},
  isbn = {978-1-5090-0608-3},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/NH9QWMXX/Veganzones et al. - 2016 - Nonnegative CP decomposition of multiangle hypersp.pdf}
}

@article{cohenCorrectingInnerFilter2016,
  title = {Correcting Inner Filter Effects, a Non Multilinear Tensor Decomposition Method},
  author = {Cohen, Jeremy Emile and Comon, Pierre and Luciani, Xavier},
  year = {2016},
  month = jan,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {150},
  pages = {29--40},
  issn = {01697439},
  doi = {10.1016/j.chemolab.2015.11.002},
  urldate = {2023-07-07},
  abstract = {Among measurement used in analytical chemistry, fluorescence spectroscopy is widely spread and its applications are numerous. To recover various information on unknown components in chemical mixtures, multilinear tensor decomposition of multiway fluorescence spectra has proven extremely powerful. However, inner filter effects induce a systematic error on measurements, disturbing the decomposition. In this paper, we fully describe a non multilinear approach to include inner filter effects in the model instead of neglecting them or correcting them by linearization methods. A theoretical framework on non multilinear tensor decomposition is developed, an algorithm to recover the factors in the decomposition is detailed, and real data computer results are reported. {\copyright} 2015 Elsevier B.V. All rights reserved.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/IJXIXTPA/Cohen et al. - 2016 - Correcting inner filter effects, a non multilinear.pdf}
}

@inproceedings{rivetModelingTimeWarping2016,
  title = {Modeling Time Warping in Tensor Decomposition},
  booktitle = {2016 {{IEEE Sensor Array}} and {{Multichannel Signal Processing Workshop}} ({{SAM}})},
  author = {Rivet, Bertrand and Cohen, Jeremy E.},
  year = {2016},
  month = jul,
  pages = {1--5},
  publisher = {IEEE},
  address = {Rio de Janerio, Brazil},
  doi = {10.1109/SAM.2016.7569733},
  urldate = {2023-07-07},
  isbn = {978-1-5090-2103-1},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/CYKD5ULY/Rivet and Cohen - 2016 - Modeling time warping in tensor decomposition.pdf}
}

@article{cohenDictionarybasedTensorCanonical2018,
  title = {Dictionary-Based {{Tensor Canonical Polyadic Decomposition}}},
  author = {Cohen, J{\'e}r{\'e}my E. and Gillis, Nicolas},
  year = {2018},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {66},
  number = {7},
  eprint = {1704.00541},
  primaryclass = {stat},
  pages = {1876--1889},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2017.2777393},
  urldate = {2023-07-07},
  abstract = {To ensure interpretability of extracted sources in tensor decomposition, we introduce in this paper a dictionarybased tensor canonical polyadic decomposition which enforces one factor to belong exactly to a known dictionary. A new formulation of sparse coding is proposed which enables high dimensional tensors dictionary-based canonical polyadic decomposition. The benefits of using a dictionary in tensor decomposition models are explored both in terms of parameter identifiability and estimation accuracy. Performances of the proposed algorithms are evaluated on the decomposition of simulated data and the unmixing of hyperspectral images.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {CoheG18:tsp sparse coding identifiability hyperspectral,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/H4VIKT2R/Cohen and Gillis - 2018 - Dictionary-based Tensor Canonical Polyadic Decompo.pdf}
}

@inproceedings{cohenNewApproachDictionarybased2017,
  title = {A New Approach to Dictionary-Based Nonnegative Matrix Factorization},
  booktitle = {2017 25th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Cohen, Jeremy E. and Gillis, Nicolas},
  year = {2017},
  month = aug,
  pages = {493--497},
  publisher = {IEEE},
  address = {Kos, Greece},
  doi = {10.23919/EUSIPCO.2017.8081256},
  urldate = {2023-07-07},
  abstract = {In this paper, we propose a new model along with an algorithm for dictionary-based nonnegative matrix factorization. We show its effectiveness on spectral unmixing of hyperspectral images using self dictionary compared to state-of-the-art methods.},
  isbn = {978-0-9928626-7-1},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/98W9WSHH/Cohen and Gillis - 2017 - A new approach to dictionary-based nonnegative mat.pdf}
}

@incollection{cohenTheoryNonnegativeTucker2017,
  title = {Some {{Theory}} on {{Non-negative Tucker Decomposition}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Cohen, Jeremy E. and Comon, Pierre and Gillis, Nicolas},
  editor = {Tichavsk{\'y}, Petr and {Babaie-Zadeh}, Massoud and Michel, Olivier J.J. and {Thirion-Moreau}, Nad{\`e}ge},
  year = {2017},
  volume = {10169},
  pages = {152--161},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-53547-0_15},
  urldate = {2023-07-07},
  abstract = {Some theoretical difficulties that arise from dimensionality reduction for tensors with non-negative coefficients is discussed in this paper. A necessary and sufficient condition is derived for a low nonnegative rank tensor to admit a non-negative Tucker decomposition with a core of the same non-negative rank. Moreover, we provide evidence that the only algorithm operating mode-wise, minimizing the dimensions of the features spaces, and that can guarantee the non-negative core to have low non-negative rank requires identifying on each mode a cone with possibly a very large number of extreme rays. To illustrate our observations, some existing algorithms that compute the non-negative Tucker decomposition are described and tested on synthetic data.},
  isbn = {978-3-319-53546-3 978-3-319-53547-0},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/LDFZXP3X/Cohen et al. - 2017 - Some Theory on Non-negative Tucker Decomposition.pdf}
}

@incollection{dantasLearningFastDictionaries2018,
  title = {Learning {{Fast Dictionaries}} for {{Sparse Representations Using Low-Rank Tensor Decompositions}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Dantas, C{\'a}ssio F. and Cohen, J{\'e}r{\'e}my E. and Gribonval, R{\'e}mi},
  editor = {Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic},
  year = {2018},
  volume = {10891},
  pages = {456--466},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93764-9_42},
  urldate = {2023-07-07},
  abstract = {A new dictionary learning model is introduced where the dictionary matrix is constrained as a sum of R Kronecker products of K terms. It offers a more compact representation and requires fewer training data than the general dictionary learning model, while generalizing Tucker dictionary learning. The proposed Higher Order Sum of Kroneckers model can be computed by merging dictionary learning approaches with the tensor Canonic Polyadic Decomposition. Experiments on image denoising illustrate the advantages of the proposed approach.},
  isbn = {978-3-319-93763-2 978-3-319-93764-9},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/Z5D622J3/Dantas et al. - 2018 - Learning Fast Dictionaries for Sparse Representati.pdf}
}

@article{cohenSpectralUnmixingMultiple2018,
  title = {Spectral {{Unmixing With Multiple Dictionaries}}},
  author = {Cohen, Jeremy E. and Gillis, Nicolas},
  year = {2018},
  month = feb,
  journal = {IEEE Geoscience and Remote Sensing Letters},
  volume = {15},
  number = {2},
  pages = {187--191},
  issn = {1545-598X, 1558-0571},
  doi = {10.1109/LGRS.2017.2779477},
  urldate = {2023-07-07},
  abstract = {Spectral unmixing aims at recovering the spectral signatures of materials, called endmembers, mixed in a hyperspectral or multispectral image, along with their abundances. A typical assumption is that the image contains one pure pixel per endmember, in which case spectral unmixing reduces to identifying these pixels. Many fully automated methods have been proposed in recent years, but little work has been done to allow users to select areas where pure pixels are present manually or using a segmentation algorithm. Additionally, in a non-blind approach, several spectral libraries may be available rather than a single one, with a fixed number (or an upper or lower bound) of endmembers to chose from each. In this paper, we propose a multiple-dictionary constrained low-rank matrix approximation model that address these two problems. We propose an algorithm to compute this model, dubbed M2PALS, and its performance is discussed on both synthetic and real hyperspectral images.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/2YWL2LFZ/Cohen and Gillis - 2018 - Spectral Unmixing With Multiple Dictionaries.pdf}
}

@incollection{cohenNonnegativePARAFAC2Flexible2018,
  title = {Nonnegative {{PARAFAC2}}: {{A Flexible Coupling Approach}}},
  shorttitle = {Nonnegative {{PARAFAC2}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Cohen, Jeremy E. and Bro, Rasmus},
  editor = {Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic},
  year = {2018},
  volume = {10891},
  pages = {89--98},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93764-9_9},
  urldate = {2023-07-07},
  abstract = {Modeling variability in tensor decomposition methods is one of the challenges of source separation. One possible solution to account for variations from one data set to another, jointly analysed, is to resort to the PARAFAC2 model. However, so far imposing constraints on the mode with variability has not been possible. In the following manuscript, a relaxation of the PARAFAC2 model is introduced, that allows for imposing nonnegativity constraints on the varying mode. An algorithm to compute the proposed flexible PARAFAC2 model is derived, and its performance is studied on both synthetic and chemometrics data.},
  isbn = {978-3-319-93763-2 978-3-319-93764-9},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/S5U7MJ5F/Cohen and Bro - 2018 - Nonnegative PARAFAC2 A Flexible Coupling Approach.pdf}
}

@incollection{cohenCurveRegisteredCoupled2018,
  title = {Curve {{Registered Coupled Low Rank Factorization}}},
  booktitle = {Latent {{Variable Analysis}} and {{Signal Separation}}},
  author = {Cohen, Jeremy Emile and Cabral Farias, Rodrigo and Rivet, Bertrand},
  editor = {Deville, Yannick and Gannot, Sharon and Mason, Russell and Plumbley, Mark D. and Ward, Dominic},
  year = {2018},
  volume = {10891},
  pages = {36--45},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-93764-9_4},
  urldate = {2023-07-07},
  abstract = {We propose an extension of the canonical polyadic (CP) tensor model where one of the latent factors is allowed to vary through data slices in a constrained way. The components of the latent factors, which we want to retrieve from data, can vary from one slice to another up to a diffeomorphism. We suppose that the diffeomorphisms are also unknown, thus merging curve registration and tensor decomposition in one model, which we call registered CP. We present an algorithm to retrieve both the latent factors and the diffeomorphism, which is assumed to be in a parametrized form. At the end of the paper, we show simulation results comparing registered CP with other models from the literature.},
  isbn = {978-3-319-93763-2 978-3-319-93764-9},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/GZC4A3DE/Cohen et al. - 2018 - Curve Registered Coupled Low Rank Factorization.pdf}
}

@inproceedings{dantasHyperspectralImageDenoising2019,
  title = {Hyperspectral {{Image Denoising}} Using {{Dictionary Learning}}},
  booktitle = {2019 10th {{Workshop}} on {{Hyperspectral Imaging}} and {{Signal Processing}}: {{Evolution}} in {{Remote Sensing}} ({{WHISPERS}})},
  author = {Dantas, Cassio F. and Cohen, Jeremy E. and Gribonval, Remi},
  year = {2019},
  month = sep,
  pages = {1--5},
  publisher = {IEEE},
  address = {Amsterdam, Netherlands},
  doi = {10.1109/WHISPERS.2019.8921110},
  urldate = {2023-07-07},
  abstract = {Hyperspectral images are corrupted by noise during their acquisition. In this work, we propose to efficiently denoise hyperspectral images under two assumptions: (i) noiseless vectorized hyperspectral images are low-rank, and (ii) image patches are sparse in a proper representation basis called dictionary. These two assumptions have already led to state-ofthe-art denoising methods, but the choice of the dictionary is ad hoc. We therefore propose to learn the dictionary from hyperspectral images, a task commonly known as dictionary learning. We show that dictionary learning approach is more efficient to denoise hyperspectral images than state-of-the-art methods with fixed dictionaries, at the cost of a slightly larger computation time.},
  isbn = {978-1-72815-294-3},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/4B4YEK6M/Dantas et al. - 2019 - Hyperspectral Image Denoising using Dictionary Lea.pdf}
}

@inproceedings{dantasLearningTensorstructuredDictionaries2019,
  title = {Learning {{Tensor-structured Dictionaries}} with {{Application}} to {{Hyperspectral Image Denoising}}},
  booktitle = {2019 27th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Dantas, Cassio F. and Cohen, Jeremy E. and Gribonval, Remi},
  year = {2019},
  month = sep,
  pages = {1--5},
  publisher = {IEEE},
  address = {A Coruna, Spain},
  doi = {10.23919/EUSIPCO.2019.8902593},
  urldate = {2023-07-07},
  abstract = {Dictionary learning, paired with sparse coding, aims at providing sparse data representations, that can be used for multiple tasks such as denoising or inpainting, as well as dimensionality reduction. However, when working with large data sets, the dictionary obtained by applying unstructured dictionary learning methods may be of considerable size, which poses both memory and computational complexity issues. In this article, we show how a previously proposed structured dictionary learning model, HO-SuKro, can be used to obtain more compact and readily-applicable dictionaries when the targeted data is a collection of multiway arrays. We introduce an efficient alternating optimization learning algorithm, describe important implementation details that have a considerable impact on both algorithmic complexity and actual speed, and showcase the proposed algorithm on a hyperspectral image denoising task.},
  isbn = {978-90-827970-3-9},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/9MGFMUMM/Dantas et al. - 2019 - Learning Tensor-structured Dictionaries with Appli.pdf}
}

@article{cohenIdentifiabilityCompleteDictionary2019,
  title = {Identifiability of {{Complete Dictionary Learning}}},
  author = {Cohen, Jeremy E. and Gillis, Nicolas},
  year = {2019},
  month = jan,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {1},
  number = {3},
  pages = {518--536},
  issn = {2577-0187},
  doi = {10.1137/18M1233339},
  urldate = {2023-07-07},
  abstract = {Sparse component analysis (SCA), also known as complete dictionary learning, is the following problem: Given an input matrix M and an integer r, find a dictionary D with r columns and a matrix B with k-sparse columns (that is, each column of B has at most k nonzero entries) such that M {\textbackslash}aprox  DB. A key issue in SCA is identifiability, that is, characterizing the conditions under which D and B are essentially unique (that is, they are unique up to permutation and scaling of the columns of D and rows of B). Although SCA has been vastly investigated in the last two decades, only a few works have tackled this issue in the deterministic scenario, and no work provides reasonable bounds in the minimum number of samples (that is, columns of M ) that leads to identifiability. In this work, we provide new results in the deterministic scenario when the data has a low-rank structure, that is, when D is (under)complete. While previous bounds feature a combinatorial term {\textbackslash}bigl( kr{\textbackslash}bigr) , we exhibit a sufficient condition involving {\textbackslash}scrO (r3/(r  - k)2) samples that yields an essentially unique decomposition, as long as these data points are well spread among the subspaces spanned by r  -  1 columns of D. We also exhibit a necessary lower bound on the number of samples that contradicts previous results in the literature when k equals r  -  1. Our bounds provide a drastic improvement compared to the state of the art, and imply, for example, that for a fixed proportion of zeros (constant and independent of r, e.g., 10{\textbackslash}\% of zero entries in B), one only requires {\textbackslash}scrO (r) data points to guarantee identifiability.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/GPB76S86/Cohen and Gillis - 2019 - Identifiability of Complete Dictionary Learning.pdf}
}

@inproceedings{cohenNonnegativeLowrankSparse2019,
  title = {Nonnegative {{Low-rank Sparse Component Analysis}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cohen, Jeremy E. and Gillis, Nicolas},
  year = {2019},
  month = may,
  pages = {8226--8230},
  publisher = {IEEE},
  address = {Brighton, United Kingdom},
  doi = {10.1109/ICASSP.2019.8682188},
  urldate = {2023-07-07},
  abstract = {In this paper we consider a variant of the dictionary learning problem where the dictionary has full rank, the coefficients have a fixed sparsity level, and both the coefficients and the dictionary are nonnegative. It is equivalent to k-sparse nonnegative matrix factorization (K-NMF). This model is encountered in source separation where nonnegative linear combinations of a few components generate the data points (samples), such as in hyperspectral images. We first discuss the impact of nonnegativity on the identifiability of low-rank sparse component analysis (LRSCA), building upon recent advances. Then, as a main contribution, we propose two algorithms to train K-NMF: one based on alternating optimization and exact sparse coding, the other based on a nonnegative variant of K-subspace. We show on noiseless simulated data that our methods outperform by a large margin the state of the art. Finally, we apply our methods for the spectral unmixing of a hyperspectral image.},
  isbn = {978-1-4799-8131-1},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/CT9EQKDK/Cohen and Gillis - 2019 - Nonnegative Low-rank Sparse Component Analysis.pdf}
}

@article{cohenRecentAdvancesFast,
  title = {Recent {{Advances}} on {{Fast Dense Nonnegative Tensor Factorization}}},
  author = {Cohen, Jeremy E},
  abstract = {This short paper summarizes some recent developments for computing Nonnegative Tensor Factorization (NTF) as fast as possible. In particular, four approaches are described: tensor compression, parallel computing, randomized sampling and extrapolation. We will simply describe the basics of these approaches because of paper length restriction.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/P2PKIWCR/Cohen - Recent Advances on Fast Dense Nonnegative Tensor F.pdf}
}

@inproceedings{lorentemurHandlingNegativePatterns2019,
  title = {Handling Negative Patterns for Fast Single-Pixel Lifetime Imaging},
  booktitle = {Molecular-{{Guided Surgery}}: {{Molecules}}, {{Devices}}, and {{Applications V}}},
  author = {Lorente Mur, Antonio and Ochoa, Marien and Cohen, Jeremy and Intes, Xavier and Ducros, Nicolas},
  editor = {Pogue, Brian W. and Gioux, Sylvain},
  year = {2019},
  month = mar,
  pages = {9},
  publisher = {SPIE},
  address = {San Francisco, United States},
  doi = {10.1117/12.2511123},
  urldate = {2023-07-07},
  abstract = {Pattern generalization was proposed recently as an avenue to increase the acquisition speed of single-pixel imaging setups. This approach consists of designing some positive patterns that reproduce the target patterns with negative values through linear combinations. This avoids the typical burden of acquiring the positive and negative parts of each of the target patterns, which doubles the acquisition time. In this study, we consider the generalization of the Daubechies wavelet patterns and compare images reconstructed using our approach and using the regular splitting approach. Overall, the reduction in the number of illumination patterns should facilitate the implementation of compressive hyperspectral lifetime imaging for fluorescence-guided surgery.},
  isbn = {978-1-5106-2366-8 978-1-5106-2367-5},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/HWYYDSZ5/Lorente Mur et al. - 2019 - Handling negative patterns for fast single-pixel l.pdf}
}

@article{elviraContinuousDictionariesMeet,
  title = {Continuous Dictionaries Meet Low-Rank Tensor Approximations},
  author = {Elvira, Clement and Cohen, Jeremy E and Herzet, Cedric and Gribonval, Remi},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/DQJ64AC8/Elvira et al. - Continuous dictionaries meet low-rank tensor appro.pdf}
}

@inproceedings{shunangExtrapolatedAlternatingAlgorithms2020,
  title = {Extrapolated {{Alternating Algorithms}} for {{Approximate Canonical Polyadic Decomposition}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Shun Ang, Andersen Man and Cohen, Jeremy E. and Khanh Hien, Le Thi and Gillis, Nicolas},
  year = {2020},
  month = may,
  pages = {3147--3151},
  publisher = {IEEE},
  address = {Barcelona, Spain},
  doi = {10.1109/ICASSP40776.2020.9053849},
  urldate = {2023-07-07},
  abstract = {Tensor decompositions have become a central tool in machine learning to extract interpretable patterns from multiway arrays of data. However, computing the approximate Canonical Polyadic Decomposition (aCPD), one of the most important tensor decomposition model, remains a challenge. In this work, we propose several algorithms based on extrapolation that improve over existing alternating methods for aCPD. We show on several simulated and real data sets that carefully designed extrapolation can significantly improve the convergence speed hence reduce the computational time, especially in difficult scenarios.},
  isbn = {978-1-5090-6631-5},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/5S4BIM4Q/Shun Ang et al. - 2020 - Extrapolated Alternating Algorithms for Approximat.pdf}
}

@article{manshunangAcceleratingBlockCoordinate2021,
  title = {Accelerating Block Coordinate Descent for Nonnegative Tensor Factorization},
  author = {Man Shun Ang, Andersen and Cohen, Jeremy E. and Gillis, Nicolas and Thi Khanh Hien, Le},
  year = {2021},
  month = oct,
  journal = {Numerical Linear Algebra with Applications},
  volume = {28},
  number = {5},
  issn = {1070-5325, 1099-1506},
  doi = {10.1002/nla.2373},
  urldate = {2023-07-07},
  abstract = {This paper is concerned with improving the empirical convergence speed of blockcoordinate descent algorithms for approximate nonnegative tensor factorization (NTF). We propose an extrapolation strategy in-between block updates, referred to as heuristic extrapolation with restarts (HER). HER significantly accelerates the empirical convergence speed of most existing block-coordinate algorithms for dense NTF, in particular for challenging computational scenarios, while requiring a negligible additional computational budget.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/HWJN9BCT/Man Shun Ang et al. - 2021 - Accelerating block coordinate descent for nonnegat.pdf}
}

@inproceedings{schenkerOptimizationFrameworkRegularized2021,
  title = {An {{Optimization Framework}} for {{Regularized Linearly Coupled Matrix-Tensor Factorization}}},
  booktitle = {2020 28th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Schenker, Carla and Cohen, Jeremy E. and Acar, Evrim},
  year = {2021},
  month = jan,
  pages = {985--989},
  publisher = {IEEE},
  address = {Amsterdam, Netherlands},
  doi = {10.23919/Eusipco47968.2020.9287459},
  urldate = {2023-07-07},
  abstract = {An effective way of jointly analyzing data from multiple sources, in other words, data fusion, is to formulate the problem as a coupled matrix and tensor factorization (CMTF) problem. However, one major challenge in data fusion is that due to eclectic characteristics of data stemming from different sources, various constraints and different types of coupling between data sets should be incorporated. In this paper, we propose a flexible and efficient algorithmic framework building onto Alternating Optimization (AO) and Alternating Direction Method of Multipliers (ADMM) for coupled matrix and tensor factorizations incorporating a variety of constraints and coupling with linear transformations. Numerical experiments demonstrate that the proposed approach is accurate, computationally efficient with comparable or better performance than available CMTF methods while being also more flexible.},
  isbn = {978-90-827970-5-3},
  langid = {english}
}

@article{schenkerFlexibleOptimizationFramework2021,
  title = {A {{Flexible Optimization Framework}} for {{Regularized Matrix-Tensor Factorizations With Linear Couplings}}},
  author = {Schenker, Carla and Cohen, Jeremy E. and Acar, Evrim},
  year = {2021},
  month = apr,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {15},
  number = {3},
  pages = {506--521},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2020.3045848},
  urldate = {2023-07-07},
  abstract = {Coupled matrix and tensor factorizations (CMTF) are frequently used to jointly analyze data from multiple sources, also called data fusion. However, different characteristics of datasets stemming from multiple sources pose many challenges in data fusion and require to employ various regularizations, constraints, loss functions and different types of coupling structures between datasets. In this paper, we propose a flexible algorithmic framework for coupled matrix and tensor factorizations which utilizes Alternating Optimization (AO) and the Alternating Direction Method of Multipliers (ADMM). The framework facilitates the use of a variety of constraints, loss functions and couplings with linear transformations in a seamless way. Numerical experiments on simulated and real datasets demonstrate that the proposed approach is accurate, and computationally efficient with comparable or better performance than available CMTF methods for Frobenius norm loss, while being more flexible. Using Kullback-Leibler divergence on count data, we demonstrate that the algorithm yields accurate results also for other loss functions.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/PPXKYL8V/Schenker et al. - 2021 - A Flexible Optimization Framework for Regularized .pdf;/home/cohen/Travail/Articles/storage/WSP43CSZ/Schenker et al. - 2021 - A Flexible Optimization Framework for Regularized .pdf}
}

@inproceedings{marmoretUncoveringAudioPatterns,
  title = {Uncovering {{Audio Patterns}} in {{Music}} with {{Nonnegative Tucker Decomposition}} for {{Structural Segmentation}}},
  author = {Marmoret, Axel and Cohen, J{\'e}r{\'e}my and Bertin, Nancy and Bimbot, Fr{\'e}d{\'e}ric},
  abstract = {Recent work has proposed the use of tensor decomposition to model repetitions and to separate tracks in loopbased electronic music. The present work investigates further on the ability of Nonnegative Tucker Decompositon (NTD) to uncover musical patterns and structure in pop songs in their audio form. Exploiting the fact that NTD tends to express the content of bars as linear combinations of a few patterns, we illustrate the ability of the decomposition to capture and single out repeated motifs in the corresponding compressed space, which can be interpreted from a musical viewpoint. The resulting features also turn out to be efficient for structural segmentation, leading to experimental results on the RWC Pop data set which are potentially challenging state-of-the-art approaches that rely on extensive example-based learning schemes.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/IW7ME8LD/Marmoret et al. - Uncovering Audio Patterns in Music with Nonnegativ.pdf}
}

@article{cohenComputingProximalOperator,
  title = {Computing the Proximal Operator of the 1 Induced Matrix Norm},
  author = {Cohen, J E},
  abstract = {In this short article, for any matrix X {$\in$} Rn{\texttimes}m the proximity operator of two induced norms X 1 and X {$\infty$} are derived. Although no close form expression is obtained, an algorithmic procedure is described which costs roughly O(nm). This algorithm relies on a bisection on a real parameter derived from the Karush-Kuhn-Tucker conditions, following the proof idea of the proximal operator of the max function found in [6].},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/DRJ9T6F2/Cohen - Computing the proximal operator of the 1 induced m.pdf}
}

@inproceedings{nadisicExactSparseNonnegative2020,
  title = {Exact {{Sparse Nonnegative Least Squares}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Nadisic, Nicolas and Vandaele, Arnaud and Gillis, Nicolas and Cohen, Jeremy E.},
  year = {2020},
  month = may,
  pages = {5395--5399},
  publisher = {IEEE},
  address = {Barcelona, Spain},
  doi = {10.1109/ICASSP40776.2020.9053295},
  urldate = {2023-07-07},
  abstract = {We propose a novel approach to solve exactly the sparse nonnegative least squares problem, under hard 0 sparsity constraints. This approach is based on a dedicated branch-and-bound algorithm. This simple strategy is able to compute the optimal solution even in complicated cases such as noisy or ill-conditioned data, where traditional approaches fail. We also show that our algorithm scales well, despite the combinatorial nature of the problem. We illustrate the advantages of the proposed technique on synthetic data sets, as well as a real-world hyperspectral image.},
  isbn = {978-1-5090-6631-5},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/R7YY6YJ7/Nadisic et al. - 2020 - Exact Sparse Nonnegative Least Squares.pdf}
}

@incollection{nadisicSparseSeparableNonnegative2021,
  title = {Sparse {{Separable Nonnegative Matrix Factorization}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nadisic, Nicolas and Vandaele, Arnaud and Cohen, Jeremy E. and Gillis, Nicolas},
  editor = {Hutter, Frank and Kersting, Kristian and Lijffijt, Jefrey and Valera, Isabel},
  year = {2021},
  volume = {12457},
  pages = {335--350},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-67658-2_20},
  urldate = {2023-07-07},
  abstract = {We propose a new variant of nonnegative matrix factorization (NMF), combining separability and sparsity assumptions. Separability requires that the columns of the first NMF factor are equal to columns of the input matrix, while sparsity requires that the columns of the second NMF factor are sparse. We call this variant sparse separable NMF (SSNMF), which we prove to be NP-complete, as opposed to separable NMF which can be solved in polynomial time. The main motivation to consider this new model is to handle underdetermined blind source separation problems, such as multispectral image unmixing. We introduce an algorithm to solve SSNMF, based on the successive nonnegative projection algorithm (SNPA, an effective algorithm for separable NMF), and an exact sparse nonnegative least squares solver. We prove that, in noiseless settings and under mild assumptions, our algorithm recovers the true underlying sources. This is illustrated by experiments on synthetic data sets and the unmixing of a multispectral image.},
  isbn = {978-3-030-67657-5 978-3-030-67658-2},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/H7JRKGRV/Nadisic et al. - 2021 - Sparse Separable Nonnegative Matrix Factorization.pdf}
}

@inproceedings{nadisicExactBiobjectiveKSparse2021,
  title = {Exact {{Biobjective}} K-{{Sparse Nonnegative Least Squares}}},
  booktitle = {2021 29th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Nadisic, Nicolas and Vandaele, Arnaud and Gillis, Nicolas and Cohen, Jeremy E.},
  year = {2021},
  month = aug,
  pages = {2079--2083},
  publisher = {IEEE},
  address = {Dublin, Ireland},
  doi = {10.23919/EUSIPCO54536.2021.9616202},
  urldate = {2023-07-07},
  abstract = {The k-sparse nonnegative least squares (NNLS) problem is a variant of the standard least squares problem, where the solution is constrained to be nonnegative and to have at most k nonzero entries. Several methods exist to tackle this NP-hard problem, including fast but approximate heuristics, and exact methods based on brute-force or branch-and-bound algorithms. Although intuitive, the k-sparse constraint is sometimes limited; the parameter k can be hard to tune, especially in the case of NNLS with multiple right-hand sides (MNNLS) where the relevant k could differ between columns. In this work, we propose a novel biobjective formulation of the k-sparse nonnegative least squares problem. We present an extension of Arborescent, a branch-and-bound algorithm for exact k-sparse NNLS, that computes the whole Pareto front (that is, the set of optimal solutions for all values of k) instead of only the k-sparse solution, for virtually the same computing cost. We also present a method for MNNLS that enforces a matrix-wise sparsity constraint, by first computing the Pareto front for each column and then selecting one solution per column to build a globally optimal solution matrix. We show the advantages of the proposed approach for the unmixing of hyperspectral images.},
  isbn = {978-90-827970-6-0},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/C97JJBF3/Nadisic et al. - 2021 - Exact Biobjective k-Sparse Nonnegative Least Squar.pdf}
}

@misc{roaldPARAFAC2AOADMMConstraints2021,
  title = {{{PARAFAC2 AO-ADMM}}: {{Constraints}} in All Modes},
  shorttitle = {{{PARAFAC2 AO-ADMM}}},
  author = {Roald, Marie and Schenker, Carla and Cohen, Jeremy E. and Acar, Evrim},
  year = {2021},
  month = feb,
  number = {arXiv:2102.02087},
  eprint = {2102.02087},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-07-07},
  abstract = {The PARAFAC2 model provides a flexible alternative to the popular CANDECOMP/PARAFAC (CP) model for tensor decompositions. Unlike CP, PARAFAC2 allows factor matrices in one mode (i.e., evolving mode) to change across tensor slices, which has proven useful for applications in different domains such as chemometrics, and neuroscience. However, the evolving mode of the PARAFAC2 model is traditionally modelled implicitly, which makes it challenging to regularise it. Currently, the only way to apply regularisation on that mode is with a flexible coupling approach, which finds the solution through regularised least-squares subproblems. In this work, we instead propose an alternating direction method of multipliers (ADMM)-based algorithm for fitting PARAFAC2 and widen the possible regularisation penalties to any proximable function. Our numerical experiments demonstrate that the proposed ADMMbased approach for PARAFAC2 can accurately recover the underlying components from simulated data while being both computationally efficient and flexible in terms of imposing constraints.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/5ZVT3U2Z/Roald et al. - 2021 - PARAFAC2 AO-ADMM Constraints in all modes.pdf}
}

@article{cohenDictionaryBasedLowRankApproximations2022,
  title = {Dictionary-{{Based Low-Rank Approximations}} and the {{Mixed Sparse Coding Problem}}},
  author = {Cohen, Jeremy E.},
  year = {2022},
  month = feb,
  journal = {Frontiers in Applied Mathematics and Statistics},
  volume = {8},
  pages = {801650},
  issn = {2297-4687},
  doi = {10.3389/fams.2022.801650},
  urldate = {2023-07-07},
  abstract = {Constrained tensor and matrix factorization models allow to extract interpretable patterns from multiway data. Therefore crafting efficient algorithms for constrained low-rank approximations is nowadays an important research topic. This work deals with columns of factor matrices of a low-rank approximation being sparse in a known and possibly overcomplete basis, a model coined as Dictionary-based Low-Rank Approximation (DLRA). While earlier contributions focused on finding factor columns inside a dictionary of candidate columns, i.e., one-sparse approximations, this work is the first to tackle DLRA with sparsity larger than one. I propose to focus on the sparse-coding subproblem coined Mixed Sparse-Coding (MSC) that emerges when solving DLRA with an alternating optimization strategy. Several algorithms based on sparse-coding heuristics (greedy methods, convex relaxations) are provided to solve MSC. The performance of these heuristics is evaluated on simulated data. Then, I show how to adapt an efficient MSC solver based on the LASSO to compute Dictionary-based Matrix Factorization and Canonical Polyadic Decomposition in the context of hyperspectral image processing and chemometrics. These experiments suggest that DLRA extends the modeling capabilities of low-rank approximations, helps reducing estimation variance and enhances the identifiability and interpretability of estimated factors.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/NU5R75PZ/Cohen - 2022 - Dictionary-Based Low-Rank Approximations and the M.pdf}
}

@article{cornilletFactorisationMatricesNon,
  title = {{Factorisation en Matrices Non n{\'e}gatives bas{\'e}e Dictionnaire avec l'aide du transport optimal}},
  author = {Cornillet, R{\'e}mi and Cohen, J{\'e}r{\'e}my E and Courty, Nicolas},
  abstract = {Nonnegative Matrix Factorization (NMF) is a well known method for dimensionality reduction and low-rank approximation that allows to represent data as linear nonnegative combination of interpretable templates. In a semi-supervised setting where templates are distributions and should be picked from the columns of a known dictionary matrix, a tool of choice to define adequate metrics between templates and the dictionary is optimal transport. In this article, the Wasserstein Dictionary-based NMF (WDNMF) model is introduced and its use in spectral unmixing of hyperspectral images is discussed. A bloc coordinate descent algorithm is proposed, which performance is studied on a semi-synthetic remote sensing dataset. In these experiments, WDNMF is more robust to perturbations but performs relatively poorly compared to its competitors.},
  langid = {french},
  file = {/home/cohen/Travail/Articles/storage/JKQQULZB/Cornillet et al. - Factorisation en Matrices Non négatives basée Dict.pdf}
}

@article{juttenSourceSeparationPhysicalChemical,
  title = {Source {{Separation}} in {{Physical-Chemical Sensing}}},
  author = {Jutten, Christian and Duarte, Leonardo Tomazeli and Moussaoui, Sa{\"i}d},
  abstract = {Tensors of order d may be seen as arrays of entries indexed by d indices. They naturally appear as data arrays in applications such as chemistry, food science, forensics, environmental analysis and many other fields. Extracting and visualizing the underlying features from tensors is an important source separation problem. This chapter first describes an important class of data mining methods for tensors, namely lowrank tensor approximations (CPD, Tucker3) in the case of order d = 3. In such a case, striking differences already exist compared to low-rank approximations of matrices, which are tensors of order d = 2. Constrained decompositions and coupled decompositions, which are important variants of tensor decompositions, are also discussed in details, along with practical learning algorithms. Finally, tensor decompositions are illustrated as a tool for source separation in food sciences. In particular fluorescence spectroscopy, electrophoresis in gel, or chromatography especially coupled with mass spectrometry, are techniques where tensor decompositions are known to be useful. Some of the many other source separation problems that may be tackled with tensor decompositions are briefly discussed in the concluding remarks.},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/WEAEJSSY/Jutten et al. - Source Separation in Physical-Chemical Sensing.pdf}
}

@article{roaldAOADMMApproachConstraining2022a,
  title = {An {{AO-ADMM}} Approach to Constraining {{PARAFAC2}} on All Modes},
  author = {Roald, Marie and Schenker, Carla and Calhoun, Vince D. and Adal{\i}, T{\"u}lay and Bro, Rasmus and Cohen, Jeremy E. and Acar, Evrim},
  year = {2022},
  month = sep,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {4},
  number = {3},
  eprint = {2110.01278},
  primaryclass = {cs, math, stat},
  pages = {1191--1222},
  issn = {2577-0187},
  doi = {10.1137/21M1450033},
  urldate = {2023-07-07},
  abstract = {Analyzing multi-way measurements with variations across one mode of the dataset is a challenge in various fields including data mining, neuroscience and chemometrics. For example, measurements may evolve over time or have unaligned time profiles. The PARAFAC2 model has been successfully used to analyze such data by allowing the underlying factor matrices in one mode (i.e., the evolving mode) to change across slices. The traditional approach to fit a PARAFAC2 model is to use an alternating least squares-based algorithm, which handles the constant cross-product constraint of the PARAFAC2 model by implicitly estimating the evolving factor matrices. This approach makes imposing regularization on these factor matrices challenging. There is currently no algorithm to flexibly impose such regularization with general penalty functions and hard constraints. In order to address this challenge and to avoid the implicit estimation, in this paper, we propose an algorithm for fitting PARAFAC2 based on alternating optimization with the alternating direction method of multipliers (AO-ADMM). With numerical experiments on simulated data, we show that the proposed PARAFAC2 AO-ADMM approach allows for flexible constraints, recovers the underlying patterns accurately, and is computationally efficient compared to the state-of-the-art. We also apply our model to two real-world datasets from neuroscience and chemometrics, and show that constraining the evolving mode improves the interpretability of the extracted patterns.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {15A69 90C26,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/IUSP2XAC/Roald et al. - 2022 - An AO-ADMM approach to constraining PARAFAC2 on al.pdf}
}

@article{cohenRegularisationImpliciteFactorisations,
  title = {{R{\'e}gularisation implicite des factorisations de faible rang p{\'e}nalis{\'e}es}},
  author = {Cohen, J{\'e}r{\'e}my E and Lyon, Univ and Etienne, UJM-Saint},
  abstract = {Scale invariance is a well-known property of matrix and tensor factorization models. It is usually considered only as a source of ambiguity during inference. However, when regularizations which are not scale-invariant are added to the cost function, scale-invariance induces an implicit regularization that balances the estimated factors. These behaviors have been partially documented formally, but have not been accounted for practically. In this work, I further discuss this implicit regularization and show empirically how to adapt existing algorithms for improved robustness to hyperparameter choice and improved precision.},
  langid = {french},
  file = {/home/cohen/Travail/Articles/storage/6VWMSYN2/Cohen et al. - Régularisation implicite des factorisations de fai.pdf}
}

@misc{marmoretNonnegativeTuckerDecomposition2022,
  title = {Nonnegative {{Tucker Decomposition}} with {{Beta-divergence}} for {{Music Structure Analysis}} of {{Audio Signals}}},
  author = {Marmoret, Axel and Voorwinden, Florian and Leplat, Valentin and Cohen, J{\'e}r{\'e}my E. and Bimbot, Fr{\'e}d{\'e}ric},
  year = {2022},
  month = aug,
  number = {arXiv:2110.14434},
  eprint = {2110.14434},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.14434},
  urldate = {2023-07-07},
  abstract = {Nonnegative Tucker decomposition (NTD), a tensor decomposition model, has received increased interest in the recent years because of its ability to blindly extract meaningful patterns, in particular in Music Information Retrieval. Nevertheless, existing algorithms to compute NTD are mostly designed for the Euclidean loss. This work proposes a multiplicative updates algorithm to compute NTD with the beta-divergence loss, often considered a better loss for audio processing. We notably show how to implement efficiently the multiplicative rules using tensor algebra. Finally, we show on a music structure analysis task that unsupervised NTD fitted with beta-divergence loss outperforms earlier results obtained with the Euclidean loss.},
  archiveprefix = {arXiv},
  keywords = {15-04,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,G.1.6,H.5.5,Mathematics - Numerical Analysis},
  file = {/home/cohen/Travail/Articles/storage/BFLXZ945/Marmoret et al. - 2022 - Nonnegative Tucker Decomposition with Beta-diverge.pdf;/home/cohen/Travail/Articles/storage/P82BCD3W/2110.html}
}

@inproceedings{angAcceleratingApproximateNonnegative,
  title = {Accelerating {{Approximate Nonnegative Canonical Polyadic Decomposition}} Using {{Extrapolation}}},
  author = {Ang, Andersen and Cohen, Jeremy E and Gillis, Nicolas},
  copyright = {All rights reserved},
  langid = {english},
  file = {/home/cohen/Travail/Articles/storage/DF7GHKV6/Ang et al. - Accelerating Approximate Nonnegative Canonical Pol.pdf}
}

@phdthesis{cohenEnvironmentalMultiwayData2016,
  title = {Environmental {{Multiway Data Mining}}},
  author = {Cohen, J{\'e}r{\'e}my E},
  year = {2016},
  copyright = {All rights reserved},
  langid = {english},
  school = {Universit{\'e} Grenoble Alpes},
  file = {/home/cohen/Travail/Articles/storage/BNP6XLEM/Cohen - Environmental Multiway Data Mining.pdf}
}

@article{cohenNotationsMultiwayArray2015,
  title = {About {{Notations}} in {{Multiway Array Processing}}},
  author = {Cohen, Jeremy E},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.01306},
  eprint = {1511.01306},
  archiveprefix = {arXiv},
  file = {/home/cohen/Travail/Articles/storage/TXMZQXA8/Cohen - About Notations in Multiway Array Processing.pdf}
}

@article{georgievSparseComponentAnalysis2005,
  title = {Sparse Component Analysis and Blind Source Separation of Underdetermined Mixtures},
  author = {Georgiev, Pando and Theis, Fabian and Cichocki, Andrzej},
  year = {2005},
  journal = {IEEE transactions on neural networks},
  volume = {16},
  number = {4},
  pages = {992--996},
  publisher = {IEEE}
}

@article{cohenRegularisationImpliciteFactorisationsa,
  title = {{R{\'e}gularisation implicite des factorisations de faible rang p{\'e}nalis{\'e}es}},
  author = {Cohen, J{\'e}r{\'e}my E and Lyon, Univ and Etienne, UJM-Saint},
  abstract = {Scale invariance is a well-known property of matrix and tensor factorization models. It is usually considered only as a source of ambiguity during inference. However, when regularizations which are not scale-invariant are added to the cost function, scale-invariance induces an implicit regularization that balances the estimated factors. These behaviors have been partially documented formally, but have not been accounted for practically. In this work, I further discuss this implicit regularization and show empirically how to adapt existing algorithms for improved robustness to hyperparameter choice and improved precision.},
  langid = {french}
}

@article{huangFlexibleEfficientAlgorithmic2016,
  title = {A {{Flexible}} and {{Efficient Algorithmic Framework}} for {{Constrained Matrix}} and {{Tensor Factorization}}},
  author = {Huang, Kejun and Sidiropoulos, Nicholas D. and Liavas, Athanasios P.},
  year = {2016},
  month = oct,
  journal = {IEEE Transactions on Signal Processing},
  volume = {64},
  number = {19},
  eprint = {1506.04209},
  primaryclass = {cs, math, stat},
  pages = {5052--5065},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2016.2576427},
  urldate = {2023-07-27},
  abstract = {We propose a general algorithmic framework for constrained matrix and tensor factorization, which is widely used in machine learning. The new framework is a hybrid between alternating optimization (AO) and the alternating direction method of multipliers (ADMM): each matrix factor is updated in turn, using ADMM, hence the name AO-ADMM. This combination can naturally accommodate a great variety of constraints on the factor matrices, and almost all possible loss measures for the fitting. Computation caching and warm start strategies are used to ensure that each update is evaluated efficiently, while the outer AO framework exploits recent developments in block coordinate descent (BCD)-type methods which help ensure that every limit point is a stationary point, as well as faster and more robust convergence in practice. Three special cases are studied in detail: non-negative matrix/tensor factorization, constrained matrix/tensor completion, and dictionary learning. Extensive simulations and experiments with real data are used to showcase the effectiveness and broad applicability of the proposed framework.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {AO-ADMM,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/FZWX54AH/Huang et al. - 2016 - A Flexible and Efficient Algorithmic Framework for.pdf}
}

@misc{gribonvalSparseSpuriousDictionary2015,
  title = {Sparse and Spurious: Dictionary Learning with Noise and Outliers},
  shorttitle = {Sparse and Spurious},
  author = {Gribonval, R{\'e}mi and Jenatton, Rodolphe and Bach, Francis},
  year = {2015},
  month = aug,
  number = {arXiv:1407.5155},
  eprint = {1407.5155},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-10-13},
  abstract = {A popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary. While this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing, there have only been a few theoretical arguments supporting these evidences. In particular, sparse coding, or sparse dictionary learning, relies on a non-convex procedure whose local minima have not been fully analyzed yet. In this paper, we consider a probabilistic model of sparse signals, and show that, with high probability, sparse coding admits a local minimum around the reference dictionary generating the signals. Our study takes into account the case of over-complete dictionaries, noisy signals, and possible outliers, thus extending previous work limited to noiseless settings and/or under-complete dictionaries. The analysis we conduct is non-asymptotic and makes it possible to understand how the key quantities of the problem, such as the coherence or the level of noise, can scale with respect to the dimension of the signals, the number of atoms, the sparsity and the number of observations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/M6295QUY/Gribonval et al. - 2015 - Sparse and spurious dictionary learning with nois.pdf}
}

@article{cohenRegularisationImpliciteFactorisations2023a,
  title = {{R{\'e}gularisation implicite des factorisations de faible rang p{\'e}nalis{\'e}es}},
  author = {Cohen, J{\'e}r{\'e}my E},
  year = {2023},
  journal = {GRETSI 2023, Grenoble},
  abstract = {Scale invariance is a well-known property of matrix and tensor factorization models. It is usually considered only as a source of ambiguity during inference. However, when regularizations which are not scale-invariant are added to the cost function, scale-invariance induces an implicit regularization that balances the estimated factors. These behaviors have been partially documented formally, but have not been accounted for practically. In this work, I further discuss this implicit regularization and show empirically how to adapt existing algorithms for improved robustness to hyperparameter choice and improved precision.},
  langid = {french},
  file = {/home/cohen/Travail/Articles/storage/ZFAT8RVG/Cohen - Régularisation implicite des factorisations de fai.pdf}
}

@misc{abdolaliDualSimplexVolume2024,
  title = {Dual {{Simplex Volume Maximization}} for {{Simplex-Structured Matrix Factorization}}},
  author = {Abdolali, Maryam and Barbarino, Giovanni and Gillis, Nicolas},
  year = {2024},
  month = mar,
  number = {arXiv:2403.20197},
  eprint = {2403.20197},
  primaryclass = {cs, eess, math, stat},
  publisher = {arXiv},
  urldate = {2024-04-04},
  abstract = {Simplex-structured matrix factorization (SSMF) is a generalization of nonnegative matrix factorization, a fundamental interpretable data analysis model, and has applications in hyperspectral unmixing and topic modeling. To obtain identifiable solutions, a standard approach is to find minimum-volume solutions. By taking advantage of the duality/polarity concept for polytopes, we convert minimum-volume SSMF in the primal space to a maximum-volume problem in the dual space. We first prove the identifiability of this maximum-volume dual problem. Then, we use this dual formulation to provide a novel optimization approach which bridges the gap between two existing families of algorithms for SSMF, namely volume minimization and facet identification. Numerical experiments show that the proposed approach performs favorably compared to the state-of-the-art SSMF algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Dual,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Numerical Analysis,Simplex Matrix Factorization,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/YJKMDH5I/Abdolali et al. - 2024 - Dual Simplex Volume Maximization for Simplex-Struc.pdf}
}

@misc{wuSemiSupervisedConvolutiveNMF2022,
  title = {Semi-{{Supervised Convolutive NMF}} for {{Automatic Piano Transcription}}},
  author = {Wu, Haoran and Marmoret, Axel and Cohen, J{\'e}r{\'e}my E.},
  year = {2022},
  month = apr,
  number = {arXiv:2202.04989},
  eprint = {2202.04989},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-06-18},
  abstract = {Automatic Music Transcription, which consists in transforming an audio recording of a musical performance into symbolic format, remains a difficult Music Information Retrieval task. In this work, which focuses on piano transcription, we propose a semi-supervised approach using low-rank matrix factorization techniques, in particular Convolutive Nonnegative Matrix Factorization. In the semi-supervised setting, only a single recording of each individual notes is required. We show on the MAPS dataset that the proposed semi-supervised CNMF method performs better than state-of-the-art low-rank factorization techniques and a little worse than supervised deep learning state-of-the-art methods, while however suffering from generalization issues.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,H.5.5},
  file = {/home/cohen/Travail/Articles/storage/NQ9GZ9SP/Wu et al. - 2022 - Semi-Supervised Convolutive NMF for Automatic Pian.pdf}
}

@book{gillisNonnegativeMatrixFactorization2020,
  title = {Nonnegative {{Matrix Factorization}}},
  author = {Gillis, Nicolas},
  year = {2020},
  month = jan,
  series = {Data {{Science}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611976410},
  urldate = {2024-06-21},
  isbn = {978-1-61197-640-3},
  keywords = {audio source separation,beta-divergences,blind source separation,block coordinate descent methods,communication complexity,computational complexity,dictionary learning,extended formulations,feature extraction,gene expression analysis,hyperspectral imaging,identifiability,linear dimensionality reduction,low-rank matrix approximations,matrix factorization,multiplicative updates,nested polytope problem,non-linear optimization,Nonnegative matrix factorization,nonnegative rank,principal component analysis,recommender systems,self-modeling curve resolution,separability,singular value decomposition,sparsity,topic modeling},
  file = {/home/cohen/Travail/Articles/storage/4RNVEB8J/Gillis - 2020 - Nonnegative Matrix Factorization.pdf}
}

@article{abdolaliSimplexStructuredMatrixFactorization2021,
  title = {Simplex-{{Structured Matrix Factorization}}: {{Sparsity-based Identifiability}} and {{Provably Correct Algorithms}}},
  shorttitle = {Simplex-{{Structured Matrix Factorization}}},
  author = {Abdolali, Maryam and Gillis, Nicolas},
  year = {2021},
  month = jan,
  journal = {SIAM Journal on Mathematics of Data Science},
  volume = {3},
  number = {2},
  eprint = {2007.11446},
  primaryclass = {cs, eess, stat},
  pages = {593--623},
  issn = {2577-0187},
  doi = {10.1137/20M1354982},
  urldate = {2024-06-21},
  abstract = {In this paper, we provide novel algorithms with identifiability guarantees for simplex-structured matrix factorization (SSMF), a generalization of nonnegative matrix factorization. Current state-of-the-art algorithms that provide identifiability results for SSMF rely on the sufficiently scattered condition (SSC) which requires the data points to be well spread within the convex hull of the basis vectors. The conditions under which our proposed algorithms recover the unique decomposition is in most cases much weaker than the SSC. We only require to have \$d\$ points on each facet of the convex hull of the basis vectors whose dimension is \$d-1\$. The key idea is based on extracting facets containing the largest number of points. We illustrate the effectiveness of our approach on synthetic data sets and hyperspectral images, showing that it outperforms state-of-the-art SSMF algorithms as it is able to handle higher noise levels, rank deficient matrices, outliers, and input data that highly violates the SSC.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/cohen/Travail/Articles/storage/VS3UWICW/Abdolali and Gillis - 2021 - Simplex-Structured Matrix Factorization Sparsity-.pdf;/home/cohen/Travail/Articles/storage/IYBDHI9L/2007.html}
}

@misc{liangSimpleAlternatingMinimization2022,
  title = {Simple {{Alternating Minimization Provably Solves Complete Dictionary Learning}}},
  author = {Liang, Geyu and Zhang, Gavin and Fattahi, Salar and Zhang, Richard Y.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12816},
  eprint = {2210.12816},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.12816},
  urldate = {2024-08-21},
  abstract = {This paper focuses on complete dictionary learning problem, where the goal is to reparametrize a set of given signals as linear combinations of atoms from a learned dictionary. There are two main challenges faced by theoretical and practical studies of dictionary learning: the lack of theoretical guarantees for practically-used heuristic algorithms, and their poor scalability when dealing with huge-scale datasets. Towards addressing these issues, we show that when the dictionary to be learned is orthogonal, that an alternating minimization method directly applied to the nonconvex and discrete formulation of the problem exactly recovers the ground truth. For the huge-scale, potentially online setting, we propose a minibatch version of our algorithm, which can provably learn a complete dictionary from a huge-scale dataset with minimal sample complexity, linear sparsity level, and linear convergence rate, thereby negating the need for any convex relaxation for the problem. Our numerical experiments showcase the superiority of our method compared with the existing techniques when applied to tasks on real data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Optimization and Control},
  file = {/home/cohen/Travail/Articles/storage/LBALMWAC/Liang et al. - 2022 - Simple Alternating Minimization Provably Solves Co.pdf;/home/cohen/Travail/Articles/storage/2HNL4NLE/2210.html}
}
