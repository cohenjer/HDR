{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of complete Dictionary Learning with Tensorly\n",
    "\n",
    "It is possible to compute a cDL with Tensorly using the constrained CP decomposition function. Indeed, one may compute a solution to the cDL problem by solving the optimization problem\n",
    "\n",
    "$$ \\min_{D,B} \\|M - DB\\|_F^2 + \\eta_{\\|B[:,i]\\|_0\\leq k} $$\n",
    "\n",
    "Tensorly implements an alternating optimization algorithm (AO, also called inexact block-coordinate descent) where each block (here the dictionary $D$ and the coefficients $X$ respectively) are updated using a few iterations of Alternating Descent Method of Multipliers. This algorithm was proposed as a flexible optimization framework for constrained matrix and tensor problems by Huang, Sidiropoulos and Liavas {cite}`huangFlexibleEfficientAlgorithmic2016` and implemented in Tensorly in collaboration with Caglayan Tuna in [PR#284](https://github.com/tensorly/tensorly/pull/284).\n",
    "\n",
    "This algorithm is demonstrated below on a simulated example. Convergence speed and runtime can be improved by using more dedicated software and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theorem 1 ensures identifiability with more than 16.0 data samples\n",
      "There are 32 data samples used in this experiment\n",
      "Initial cost was 0.010249795447249327, Final cost is 0.0, 500 iterations were used\n",
      "Support estimation: Precision 1.00, Recall 1.00, F metric 1.00, Accuracy 1.00\n",
      "Relative error on the dictionary: 5.1342011055608935e-14, on the sparse factor: 6.386519741193117e-14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition._constrained_cp import constrained_parafac\n",
    "from tensorly.cp_tensor import cp_permute_factors\n",
    "\n",
    "# ------------- Parameters ---------------\n",
    "k = 3 # true number of nonzeros in columns of X\n",
    "kest = 3 # number of nonzeros in the computed solution Xe\n",
    "noise = 0 # how much noise in the data\n",
    "rank = 8 # rank of the factorization (number of atoms)\n",
    "sig = 0.02 # how far from the solution we initialize\n",
    "oversampling = 2.0 # oversampling factor, <1. has lots of chances to fail\n",
    "\n",
    "# Getting dimensions from Theorem 1 bound\n",
    "bound_theorem1 = (np.floor(rank*(rank-2)/(rank-k))+1)*(rank/(rank-k))\n",
    "print(f\"Theorem 1 ensures identifiability with more than {bound_theorem1} data samples\")\n",
    "dims = [rank, int(np.floor(oversampling*bound_theorem1))]\n",
    "print(f\"There are {dims[1]} data samples used in this experiment\")\n",
    "\n",
    "# ------------ Data generation -----------\n",
    "D = np.random.randn(dims[0],rank)\n",
    "D = D/tl.norm(D,axis=0)\n",
    "X = np.random.randn(rank,dims[1])\n",
    "# sparsify X, Bernouilli Gaussian model\n",
    "for i in range(X.shape[1]):\n",
    "    X[:-k,i] = 0\n",
    "    np.random.shuffle(X[:,i])\n",
    "\n",
    "M = D@X\n",
    "Mnoise = M + noise*np.random.randn(*M.shape)\n",
    "\n",
    "# Init close to solution\n",
    "D0 = D+sig*np.random.randn(*D.shape)\n",
    "X0 = X+sig*np.random.randn(*X.shape)\n",
    "init = (None,[D0,X0.T])\n",
    "\n",
    "# ------- Decomposition with Tensorly -----\n",
    "out, err = constrained_parafac(Mnoise, rank, hard_sparsity_rowwise={1:kest}, verbose=False, init=init, n_iter_max=500, return_errors=True, tol_outer=0)\n",
    "print(f\"Initial cost was {err[0]}, Final cost is {err[-1]}, {len(err)} iterations were used\")\n",
    "# postprocess estimate by permuting the components optimally\n",
    "try:\n",
    "    out, _ = cp_permute_factors((None,[D,X.T]), out)\n",
    "except:\n",
    "    print(\"no permutation can be computed because a zero component is present in the true or estimated coefficients\")\n",
    "Xe = out[1][1].T\n",
    "De = out[1][0]\n",
    "\n",
    "# ----------- Error metrics --------------\n",
    "# Computing True False Positives, True False Negatives\n",
    "tp = tl.sum((X!=0) & (Xe!=0))\n",
    "fp = tl.sum((X==0) & (Xe!=0))\n",
    "fn = tl.sum((X!=0) & (Xe==0)) # always equal to fp if k is chosen optimally\n",
    "tn = tl.sum((X==0) & (Xe==0))\n",
    "# Precision, Recall, Accuracy, Fmetric\n",
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "accuracy = (tp+tn)/(tp+tn+fn+fp)\n",
    "fmet = 2*precision*recall/(precision+recall)\n",
    "print(f\"Support estimation: Precision {precision:.2f}, Recall {recall:.2f}, F metric {fmet:.2f}, Accuracy {accuracy:.2f}\")\n",
    "\n",
    "# Estimation of D and X\n",
    "Denorms = tl.norm(De, axis=0)\n",
    "De = De/Denorms\n",
    "Xe = (Denorms*Xe.T).T\n",
    "rmse_D = tl.norm(D-De)/tl.sqrt(tl.prod(D.shape))\n",
    "rmse_X = tl.norm(X-Xe)/tl.sqrt(tl.prod(X.shape))\n",
    "print(f\"Relative error on the dictionary: {rmse_D}, on the sparse factor: {rmse_X}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play with the parameters. One may observe for instance that as soon as the initial factors are chosen too far from the true solution, most of the time of support is poorly estimated. Similarly, the true support is only recovered in small noise regimes. These two properties, respectively local convergence and robustness, have been studied in the literature, see for instance {cite}`liangSimpleAlternatingMinimization2022` and {cite}`gribonvalSparseSpuriousDictionary2015` and references therein. However the bounds on the initial error and noise levels are in general hard to compute explicitly.\n",
    "\n",
    "We can illustrate the previously presented result on identifiability with this simulation. The data points are located uniformly on each facet with sparsity exactly $k$. Each facets requires $\\lfloor \\frac{r(r-2)}{ r-k }\\rfloor +1$ points at least and there are $r-k$ facets. Each point lives in k facets. Therefore we need more than $(\\lfloor \\frac{r(r-2)}{ r-k } \\rfloor+1)\\frac{r}{r-k}$ data points to satisfy Theorem1 (todo cross ref).\n",
    "\n",
    "In the above simulation, setting the noise level to zero, if there are fewer points than the bound of {prf:ref}`thm_identif_cDL`, the dictionary and the sparse factors are never recovered because of a lack of identifiability. This is more visible when starting midly far from the true solution. Indeed when the model is not identifiable, the true solution is still a minimizer of the problem, but there are generally infinitely many solutions which may be obtained instead. Try running the simulation several times with the following parameter set:\n",
    "\n",
    "```python\n",
    "\n",
    "    k = 3 # true number of nonzeros in columns of X\n",
    "    kest = 3 # number of nonzeros in the computed solution Xe\n",
    "    noise = 0 # how much noise in the data\n",
    "    rank = 8 # rank of the factorization (number of atoms)\n",
    "    sig = 0.03 # how far from the solution we initialize\n",
    "    oversampling = .8 # oversampling factor, <1. has lots of chances to fail\n",
    "```\n",
    "\n",
    "and observe that the RMSE are generally not small, even when perfect support estimation occurs.\n",
    "\n",
    "When more samples are drawn, because of the stochastic nature of the repartition of samples on the facets, it is possible that the model is still not identifiable, but as the number of points grows this becomes less likely. The algorithm still falls in local minimizers, but sometimes it finds a solution very close to the true, global solution. Try running the simulation with the same parameters but more samples:\n",
    "\n",
    "```python\n",
    "\n",
    "    oversampling = 2.\n",
    "\n",
    "```\n",
    "\n",
    "and notice how the RMSE on the dictionary and factor are often smaller then in the undersampled regime, and sometimes close to machine precision.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
